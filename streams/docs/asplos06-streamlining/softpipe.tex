\begin{figure}[t]
\begin{center}
\psfig{figure=pipeline-scaling.eps,width=3.2in}
\vspace{-8pt}
\end{center}
\caption{Potential speedups of pure pipeline parallelism over pure
data parallelism for varying amounts of stateful work in the
application.  Each line represents a different amount of work in the
heaviest stateful filter.  The graph assumes 16 cores and does not
consider task parallelism or communication costs.
\protect\label{fig:model-speedup}}
\end{figure}

\section{Coarse-Grained Software Pipelining}
\label{sec:softpipe}

\subsection{Benefits of Pipeline Parallelism}
\label{sec:pipeline-model}

Pipeline parallelism is an important mechanism for parallelizing
filters that have dependences from one iteration to another.  Such
``stateful'' filters are not data parallel and do not benefit from the
techniques described in the previous section.  While many streaming
applications have abundant data parallelism, even a small number of
stateful filters can greatly limit the performance of a purely
data-parallel approach on a large multicore architecture.

The potential benefits of pipeline parallelism are straightforward to
quantify.  Consider that the sequential execution of an application
requires unit time, and let $\sigma$ denote the fraction of work
(sequential execution time) that is spent within stateful filters.
Also let $\mu$ denote the maximum work performed by any individual
stateful filter.  Given $N$ processing cores, we model the execution
time achieved by two scheduling techniques: 1) data parallelism, and
2) data parallelism plus pipeline parallelism.  In this exercise, we
assume that execution time is purely a function of load balancing; we
do not model the costs of communication, synchronization, locality,
etc.  We also do not model the impact of task parallelism.

\begin{enumerate}
\item Using data parallelism, $1-\sigma$ parts of the work are
data-parallel and can be spread across all $N$ cores, yielding a
parallel execution time of $(1-\sigma)/N$.  The stateful work must be
run as a separate stage on a single core, adding $\sigma$ units to the
overall execution.  The total execution time is $\sigma+(1-\sigma)/N$.

\item Using data and pipeline parallelism, any set of filters can
execute in parallel during the steady state.  (That is, each stateful
filter can now execute in parallel with others; the stateful filter
itself is not parallelized.)  The stateful filters can be assigned to
the processors, minimizing the maximum amount of work allocated to any
processor.  Even a greedy assignment (filling up one processor at a
time) guarantees that no processor exceeds the lower-bound work
balance of $\sigma/N$ by more than $\mu$, the heaviest stateful
filter.  Thus, the stateful work can always complete in $\sigma/N+\mu$
time.  Remaining data parallelism can be freely distributed across
processors.  If it fills each processor to $\sigma/N+\mu$ or beyond,
then there is perfect utilization and execution completes in $1/N$
time; otherwise, the state is the bottleneck.  Thus the general
execution time is $\max(\sigma/N+\mu, 1/N)$.

%% \begin{itemize}

%% \item In the worst case, there is only one stateful filter.  It
%% executes on 1 core, while the other $N-1$ cores are filled with data
%% parallelism.  If $s > 1/N$, then the data-parallel work finishes
%% first, and the overall execution time is $s$.  Otherwise, at time $s$
%% the data-parallelism can be extended to the core executing stateful
%% work, thereby achieving perfect load balance throughout the
%% application.  The overall runtime is thus $\max(s, 1/N)$.

%% \item In the best case, the stateful filters can be distributed across
%% cores such that any resulting load imbalance can be compensated by the
%% data-parallelism.  Thus, all cores are fully utilized and the runtime
%% is $1/N$.

%% \end{\itemize}

\end{enumerate}

Using these modeled runtimes, Figure~\ref{fig:model-speedup}
illustrates the potential speedup of adding pipeline parallelism to a
data-parallel execution model for various values of $\mu/\sigma$ on a
16-core architecture.  In the best case, $\mu/\sigma$ approaches $0$
and the speedup is $(\sigma+(1-\sigma)/N)/\max(\sigma/N, 1/N) =
1+\sigma*(N-1)$.  For example, if there are $16$ cores and even as
little as $1/15$th of the work is stateful, then pipeline parallelism
offers potential gains of 2x.  For these parameters, the worst-case
gain ($\mu=\sigma$) is 1.8x.  The best and worst cases diverge further
for larger values of $\sigma$.

\subsection{Exploiting Pipeline Parallelism}

\begin{figure*}[t]
\begin{center}
\psfig{figure=time-proc-4-2.eps,width=6.2in}
\vspace{-6pt}
\end{center}

\caption{Comparison of hardware pipelining and software pipelining for
the Vocoder example (see Figure~\ref{fig:vocoder}).  For clarity, the
same assignment of filters to processors is used in both cases, though
software pipelining admits a more flexible set of assignments than
hardware pipelining.  In software pipelining, filters read and write
directly into buffers and communication is done at steady-state
boundaries.  The prologue schedule for software pipelining is not
shown.
\protect\label{fig:pipelining}}
\vspace{-8pt}
\end{figure*}

At any given time, pipeline-parallel actors are executing different
iterations from the original stream program.  However, the distance
between active iterations must be bounded, as otherwise the amount of
buffering required would grow toward infinity.  To leverage pipeline
parallelism, one needs to provide mechanisms for both decoupling the
schedule of each actor, and for bounding the buffer sizes.  This can
be done in either hardware or software.

In coarse-grained {\it hardware pipelining}, groups of filters are
assigned to independent processors that proceed at their own rate (see
Figure~\ref{fig:pipelining}a).  As the processors have decoupled
program counters, filters early in the pipeline can advance to a later
iteration of the program.  Buffer size is limited either by blocking
FIFO communication, or by other synchronization primitives (e.g., a
shared-memory data structure).  However, hardware pipelining entails a
performance tradeoff:

\begin{itemize}

\item If each processor executes its filters in a single repeating
pattern, then it is only beneficial to map a contiguous\footnote{In an
acyclic stream graph, a set of filters is {\it contiguous} if, in
traversing a directed path between any two filters in the set, the
only filters encountered are also within the set.} set of filters to a
given processor.  Since filters on the processor will always be at the
same iteration of the steady state, any filter missing from the
contiguous group and executing at a remote location would only
increase the latency of the processor's schedule.  The requirement of
contiguity can greatly constrain the partitioning options and thereby
worsen the load balancing.

\item To avoid the constraints of a contiguous mapping, processors
could execute filters in a dynamic, data-driven manner.  Each
processor monitors several filters and fires any who has data
available.  This allows filters to advance to different iterations of
the original stream graph even if they are assigned to the same
processing node.  However, because filters are executing out-of-order,
the communication pattern is no longer static and a more complex
flow-control mechanism (e.g., using credits) may be needed.  There is
also some overhead due to the dynamic dispatching step.

\end{itemize}

Coarse-grained {\it software pipelining} offers an alternative that
does not have the drawbacks of either of the above approaches (see
Figure~\ref{fig:pipelining}b).  Software pipelining provides
decoupling by executing two distinct schedules: a loop prologue and a
steady-state loop.  The prologue serves to advance each filter to a
different iteration of the stream graph, even if those filters are
mapped to the same core.  Because there are no dependences between
filters within an iteration of the steady-state loop, any set of
filters (contiguous or non-contiguous) can be assigned to a core.
This offers a new degree of freedom to the partitioner, thereby
enhancing the load balancing.  Also, software pipelining avoids the
overhead of the demand-driven model by executing filters in a fixed
and repeatable pattern on each core.  Buffering can be bounded by the
on-chip communication networks, without needing to resort to
software-based flow control.

\subsection{Software Pipelining Implementation}

Our software pipelining algorithm maps filters to cores in a similar
manner that traditional algorithms map instructions to ALUs.  This
transformation is enabled by an important property of the StreamIt
programming model, namely that the entire stream graph is wrapped with
an implicit outer loop.  As the granularity of software pipelining
increases from instructions to filters, one needs to consider the
implications for managing buffers and scheduling communication.  We
also describe our algorithm for mapping filters to cores, and compare
the process to conventional software pipelining.

We construct the loop prologue so as to buffer at least one steady state
of data items between each pair of dependent filters.  This allows
each filter to execute completely independently during each subsequent
iteration of the stream graph, as they are reading and writing to
buffers rather than communicating directly.  The buffers could be
stored in a variety of places, such as the local memory of the core, a
hardware FIFO, a shared on-chip cache, or an off-chip DRAM.  On Raw,
off-chip DRAM offers higher throughput than a core's local memory, so
we decided to store the buffers there.  However, we envision that
on-chip storage would be the better choice for most commodity
multicores.

As filters are reading and writing into buffers in distributed memory
banks, there needs to be a separate communication stage to shuffle
data between buffers.  Some of this communication is a direct transfer
of data, while others performs scatter or gather operations
corresponding to the splitjoins in StreamIt.  The communication stage
could be implemented by DMA engines, on-chip networks, vector
permutations, or other mechanisms.  On Raw, we leverage the
programmable static network to perform all communication in a single
stage, which is situated between iterations of the steady state.  On
architectures with DMA engines, it would be possible to parallelize
the communication stage with the computation stage by double-buffering
the I/O of each filter.

In assigning filters to cores, the goal is to optimize the load
balancing across cores while minimizing the synchronization needed
during the communication stage.  We address these criteria in two
passes, first optimizing load balancing and then optimizing the
layout.  As the load-balancing problem is NP-complete (by reduction
from SUBSET-SUM~\cite{sipser97}), we use a greedy partitioning
heuristic that assigns each filter to one of $N$ processors.  The
algorithm considers filters in order of decreasing work, assigning
each one to the processor that has the least amount of work so far.
As described in Section~\ref{sec:pipeline-model}, this heuristic
ensures that the bottleneck processor does not exceed the optimum by
more than the amount of work in the heaviest filter.

To minimize synchronization, we wrap the partitioning algorithm with a
selective fusion pass.  This pass repeatedly fuses the two adjacent
filters in the graph that have the smallest combined work.  After each
fusion step, the partitioning algorithm is re-executed; if the
bottleneck partition increases by more than a given threshold (10\%),
then the fusion is reversed and the process terminates.  This process
increases the computation to communication ratio of the stream graph,
while also leveraging the inter-node fusion optimizations mentioned
previously.  It improves performance by up to 2x on the Radar
benchmark, with a geometric mean of 15\% across all benchmarks.

Overall, coarse-grained software pipelining on a multicore
architecture avoids many of the complications and exposes many new
optimization opportunities versus traditional software pipelining.  In
traditional software pipelining, the limited size of the register file
is always an adversary (register pressure), but there is ample memory
available for buffering stream data.
%In addition, the unrolling
%associated with instruction-level software pipelining causes a
%corresponding expansion of code size; however, since we are mapping
%each filter to only a single processor, there is no code bloat.
Another recurring issue traditionally is the length of the prologue to
the software pipelined loop, but this is less problematic in the
streaming domain because the steady-state executes longer.  Lastly,
and most importantly, we can exploit these properties to fully
pipeline the graph, removing all dependences and thus removing all the
constraints for our scheduling algorithm.

%% MOVE TO CONCLUSIONS
%% 
%% Overall, it is the implicit and infinite outer loop around the stream
%% graph that enables our techniques.  We recognize this outer loop and
%% apply scheduling techniques traditionally reserved for scheduling
%% loops of machine instructions.  One can think of each filter as a
%% single instruction and each core as a functional unit of a
%% uniprocessor.  There exists data-flow dependencies between filters and
%% filters have an occupancy.  The stream graph, much like an
%% instruction-level data-flow graph, encodes the dependences between
%% filters.  Thus, our compiler maps filters to cores much like a
%% traditional scheduler maps instructions to functional units.

%%   why we don't need a modulo scheduling algorithm
%%   why we assume iteration interval of one
%%   dependencies of the graph
%%   how we model computation and communication costs
%%   how we reduce communication cost (synchronization annealing)
%%   explain the model of execution and what happens at the steady state
%%   explain the buffering between filters
%% 	approximately a 2x increase but it does down with gran adj
  
%% talk about the data-reorganization stage

%% NP-completeness, running time of algorithms, approximation of greedy
%% packing (not too important, but get name correct).

%% Overview.
%% Does not incur overhead of duplicate (data-distribution) vs. the
%% thread dup case and still has good load balancing (ideal work numbers).

%% No antidependencies, no output dependencies, no control dependencies!
%% only producer/consumer data dependencies.
%% Loop-carried dependences from each node to itself
%% So difference (dif) between producer and consumer in SS is always one
%% iteration difference is at most the max height of the graph 
%% (single source, sink)
%% Kernel is the steady-state
%% Initiation Interval is the critical path (most work tile)
%% Iteration count is always 1

%% After the software pipeline the stream graph, each computational node
%% will have a time-ordered list of filters to fire. Each filter executes
%% for the number of times given by the SAS schedule.  The first filter
%% mapped to the node will fire when its input has arrived, and when it
%% is finished, the next filter will execute.  In this way, a complete
%% software pipelined execution will unfold on the target.

%% The compiler software pipelines the {\it entire} stream graph at the
%% filter level. By this we mean that during execution of the
%% steady-state, two filters that have a dependence chain between them in
%% the filter graph are executing at different iterations of the original
%% steady-state loop.  We introduce enough buffering between the filters
%% to support this pipelined model. The filters can now execute
%% out-of-order within the steady-state, and they do not directly
%% communicate.  

%% To achieve this software pipelined schedule, we must buffer adequately
%% between filters.  Each conceptual buffer between two filters is
%% represented as a rotating set of buffers (much like a rotating
%% register file \textbf{ref?}), with a separate rotation for the
%% producer and the consumer, the producer always being ahead of the
%% consumer in the rotation.

%% In our approach, we first assume that we have a machine with infinite
%% parallel resources.  We generate a prologue schedule that will
%% guarantee that each filter is ready to execute.  In software pipelining
%% terminology, the iteration interval is 1.  This is essential because,
%% unlike machine instructions, there exists complicated
%% data-organization between filters. Therefore, we delay communication
%% and data-reorganization between filters until it can be more
%% efficiently executed.  We rely on a {\it data-reorganization
%% phase} that executes before each steady-state to perform the
%% data-reorganization described by each joiner and splitter of the filter
%% graph.  {\it Before} we execute a steady-state schedule, all joining
%% for the entire filter graph is executed using the buffers in off-chip
%% DRAMs as the sources and destinations, and using the interconnect of
%% the target to perform the data-reorganization stipulated by each
%% joiner.  {\it After} each steady-state completes we perform the analog
%% for splitters.

%% When the steady-state is repeated, the splitting and joining phases
%% happen in sequence.  First, we perform all the splitting of the graph
%% from the steady-state execution that just finished; then, we perform
%% all the joining for the steady-state execution that is about to
%% commence. Next, computation of the filters begins.  Observations early
%% in the development of the SpaceTime compiler show that this separation
%% of computation from data-reorganization has a huge benefit over a
%% combined schedule that intermixes the two.  The intuition is that we
%% need to route over many of the computational nodes to perform most
%% reorganizations, and during the steady-state other are performing the
%% computation and communication of other filters; it is very difficult to
%% coordinate this interaction efficiently.

%% The buffers that are the sources and destinations of the
%% data-reorganization stage are the rotated.  Each conceptual arc in the
%% stream graph is concretely represented by allocating enough buffers as
%% the iteration difference between the source and destination filters in
%% the steady-state.

%% Recall that the data-reorganization described by splitters and joiners
%% of the graph is implemented by the on-chip network, thus we do not map
%% these nodes to computational nodes.  Each filter lives on exactly one
%% computational node.  The two initialization schedules described below
%% use the assignment of filters to computational nodes calculated by the
%% steady-state scheduler.

%% \subsection{Peek Initialization Schedule}
%% Before both the prologue schedule is executed and the steady-state
%% schedule is commenced, we perform a {\it peek initialization
%% schedule}.  This schedule is required to make sure that we can create
%% a cyclic steady-state schedule that respects StreamIt's peeking
%% operation.  In this schedule, filters are executed in data-flow order
%% and the buffers are not rotated.  Therefore, the prologue schedule
%% starts with the rotating buffers untouched.  See
%% \cite{streamitcc} for a more complete discussion of the peek
%% initialization schedule.

%% \subsection{Prologue Schedule}
%% The prologue schedule guarantees that when the steady-state commences,
%% all the filters are ready to fire irrespective of their data-flow
%% dependencies.  This schedule is composed of multiple iterations.  At
%% each iteration we fire all the filters whose input requirements are met
%% from the previous iteration.  We stop the schedule when all filters are
%% ready to fire.

%% \subsection{Selective Fusion}
%% \begin{algorithm}
%% \caption{Selective Fusion} \label {alg:select_fus}
%% \textsc{SelectFusion}($G = (V, E), w, P$)
%% \begin{algorithmic}[1]
%% \State $(A, proc$-$weight) \gets $ \Call{WorkDist}{$G, w, P$} 
%% \State $prev$-$max \gets $ \Call{MaxProc}{$proc$-$weight$}
%% \Repeat
%% 	\State $G_{prev} \gets G$
%% 	\State $G \gets $ \Call{AdjGreedyFusion}{$G, w$}
%% 	\State $w \gets $ \Call{UpdateWeights}{G}
%% 	\State $(A, proc$-$weight) \gets $ \Call{WorkDist}{$G, w, P$}
%% 	\State $new$-$max \gets $ \Call{MaxProc}{$proc$-$weight$}
%% 	\State $change \gets prev$-$max / new$-$max $
%% 	\State $prev$-$max \gets new$-$max$
%% \Until{$change < threshold$}
%% \State \textbf{return} $G_{prev}$
%% \end{algorithmic}
%% \end{algorithm}

%% \begin{algorithm}
%% \caption{Adjacent Greedy Fusion} \label {alg:adj_fus}
%% \textsc{AdjGreedyFusion}($G = (V, E), w$)
%% \begin{algorithmic}[1]
%% \ForAll {$v \in V$}  
%% 	\ForAll {$u \in V$}
%% 		\State $pair$-$weight[u,v] \gets \infty$
%% 	\EndFor
%% \EndFor
%% \ForAll {$v \in V$}
%% \Statex // For a filter, 
%% 	\If {\Call{Filter}{$v$} $\wedge \mid$\Call{Out}{$v$}$\mid = 1$ $\wedge $ \Call{Filter}{\textsc{Out}$(v)[0]$}}
%% 		\State $u \gets $ \Call{Out}{$v$}$[0]$
%% 		\State $pair$-$weight[v,u] \gets w(v) + w(u)$
%% 	\ElsIf {\Call{Splitter}{$v$}}
%% 		\Statex // decide if this is a simple splitter
%% 		\State $simple \gets true$			
%% 		\ForAll {$d \in $\Call{Out}{$v$}}
%% 			\If {$\neg$\Call{Filter}{$d$} $ \vee  \neg$\Call{Joiner}{\textsc{Out}($d$)$[0]$}}
%% 				\State $simple \gets false$
%% 				\State \textbf{break}
%% 			\EndIf
%% 		\EndFor
%% 		\If {$simple$}
%% 			\For {$i \gets 0, \mid$\Call{Out}{$v$}$\mid - 1$}
%% 				\State $x \gets $\Call{Out}{$v$}$[i]$
%% 				\State $y \gets $\Call{Out}{$v$}$[i + 1]$
%% 				\State $pair$-$weight[x,y] \gets w(x) + w(y)$
%% 			\EndFor 
%% 		\EndIf
%% 	\EndIf
%% \EndFor
%% \Statex // Find the min adjacent pair and fuse them
%% \State $(p, q) \gets $ \Call{MinPair}{$pair$-$weight$}
%% \State $G \gets $ \Call{Fuse}{$G, p, q$}
%% \Statex // Delete any splitters that have single output
%% \State \Call{RemoveDeadSplitters}{G}
%% \Statex // Delete any joiners that have single input
%% \State \Call{RemoveDeadJoiners}{G}
%% \State \textbf{return} $G$
%% \end{algorithmic}
%% \end{algorithm}

%% Why do we only fuse components of a splitjoin when each stream of the
%% splitjoin is of height 1 (a filter)?  I know the answer, but the
%% reader might wonder why.
 
%% \subsection{Work Distribution}
%% Homogeneous, persistent, irregular, resources.

%% \begin{algorithm}
%% \caption{Work Distribution}\label{alg:work_dist}
%% %{\small Given: 
%% %\begin{itemize}
%% %\item a stream graph $G = (V, E)$
%% %\item the node weights $w$
%% %\item processors $P$
%% %\end{itemize}
%% %Return:
%% %\begin{itemize} 
%% %\item $A$, an assignment of nodes to processors.
%% %\item $T_p, p \in P,$ the sum of weights of the nodes assigned to proc
%% %$p.$
%% %\end{itemize}}
%% \textsc{WorkDist}($G(V,E), w, P$)
%% \begin{algorithmic}[1]
%% \ForAll {$p \in P$}
%% 	\State $proc$-$weight[p] \gets 0$  %\Comment{init the proc weights}
%% \EndFor
%% \State Sort $v \in V$ in descending order by $w(v)$
%% \ForAll {$v \in V$}
%%         \State $m \gets $ \Call{MinProc}{$proc$-$weight$}
%% 	\State $assign[v] \gets m$ %\Comment{Remember the assignment} 
%% 	\State $proc$-$weight[m] \gets proc$-$weight[m] + w(v)$ %\Comment{Update the work on the proc}
%% \EndFor
%% \State \textbf{return} $assign$, $proc$-$weight$ 
%% \end{algorithmic}
%% \end{algorithm}

%% \subsection{Synchronization Annealing}
